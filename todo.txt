network configuration
    decaying learning rate
    use moving average for prediction
    -change variance -> std deviation, need to restart for it to work
        bug: the model will only work with variance, very strange...
network
    increase depth of convnet to 7x7 filter output
    weight decay
-summary writing
    -training curve
    -save model periodically
distorting inputs
    based on color, hue, lighting
    -cropping
    zooming
visualize
    visualize images by class
    -visualize filters at each layer
    -visualize output of poorly performing cells
    -visualize images that respond maximally to each filter
refactor
    -evaluation should be in a separate file
    change loss function to look like the cifar-10 cross-entropy loss wit logits
    
qustions:
    why is var work but not std dev?
    how does the network go from 50x81x81x3 convolved with 5x5x3x32 = 50x81x81x32


handle distorting input on the fly
    data= load data into numpy array
    in model graph distort the batch given to the model
        placeholder = tf.flip,  tf.crop
    # get a batch from each cell, if cur_i greater than the data then shuffle and start over
    get_next_batch(cur_i, batch_size)
        for each cell_type:
                cell_type[cur_i*batch_size]
        

        


